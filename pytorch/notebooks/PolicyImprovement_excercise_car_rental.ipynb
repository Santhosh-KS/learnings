{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function $v_\\pi$ for an arbitrary deterministic policy $\\pi$. For some state s we would like to know whether or not we should change the policiy to deterministically choose an action $a \\neq \\pi(s)$. We know how good it is to follow the current policy from $s$, that is $v_\\pi(s)$ but would it be better or worse to chang to the new policy? One way to answer this question is to consider selecting $a$ in a $s$ and thereafter following the existing policy, $\\pi$. The value of this way of behaving is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\n",
    "\\begin{align}\n",
    "q_\\pi(s,a) \n",
    "  &\\doteq \\mathbb{E}\\Big[ R_{t+1} + \\gamma \\space v_\\pi(S_{t+1}) \\space | S_t=s,A_t=a \\Big]\\\\\n",
    "  &= \\sum_{s',r}p(s',r \\space | \\space s,a) \\Big[ r + \\gamma v_pi(s') \\Big] \\longrightarrow (4.6)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key criterion is whether this is greater than or less than $v_\\pi(s)$. If it is greater, that is if it is better to select $a$ once in $s$ and thereafter follow  $\\pi$ than it would be to follow $\\pi$ all the time, then one would expect it to be better still to select $a$ ever time $s$ is encountered, and that the new policy would in fact be better one overall.\n",
    ">\n",
    "That this is true is a special case of a general result called the **Policy improvement theorem**. Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all $s \\in S$,\n",
    ">\n",
    "$\\large q_\\pi(s,\\pi'(s)) \\geq \\space v_\\pi(s) \\longrightarrow (4.7)$\n",
    ">\n",
    "Then the policy $\\pi'$ must be as good as, or better than, $\\pi$. That is, it must obtain greater or equal expected return from all state $s \\in S$:\n",
    ">\n",
    "$\\large v_\\pi'(s) \\geq v_\\pi(s) \\longrightarrow (4.8)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, if there is strict inequality of (4.7) at any state, then there must be strict inequality of (4.8 at that state. This result applies in particular to the two policies that we considered in the previous paragraph, an original deterministic policy, $\\pi$, and a changed policy, $\\pi'$, that is identical to $\\pi$ except that $\\large \\pi'(s) = a \\neq \\pi(s)$, (4.7) holds at all states other than $s$. Thus, if $\\large q_\\pi(s,a) > v_\\pi(s)$, then the changed policy is indeed better than $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the proof of **policy improvement therorem** is easy to understand. Starting from (4.7), we keep expanding the $q_\\pi$ side with (4.6) and reapplying (4.7) untill we get $v_\\pi'(s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\n",
    "\\begin{align}\n",
    "v_\\pi(s)\n",
    "  &\\leq q_\\pi(s,\\pi'(s))\\\\\n",
    "  &= \\mathbb{E}\\Big[ R_{t+1} + \\gamma \\space v_\\pi(S_{t+1}) \\space | S_t=s,A_t=a \\Big] && by(4.6) \\\\\n",
    "  &= \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma \\space v_\\pi(S_{t+1}) \\space | S_t=s \\Big]\\\\\n",
    "  &\\leq \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma \\space q_\\pi(S_{t+1}, \\pi'(S_{t+1})) \\space | S_t=s \\Big] && by(4.7) \\\\\n",
    "  &= \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma \\space \\mathbb{E}_{\\pi'}\\Big[ R_{t+2} + \\gamma v_\\pi(S_{t+2}) \\space | \\space S_{t+1},A_{t+1}=\\pi'(S_{t+1})\\Big]  \\space \\Big| \\space S_t=s\\Big]\\\\\n",
    "  &= \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 v_\\pi(S_{t+2})\\space | \\space S_t=s \\Big]\\\\\n",
    "  &= \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3}+ \\gamma^3 v_\\pi(S_{t+3})\\space | \\space S_t=s \\Big]\\\\\n",
    "  & \\dots\\\\\n",
    "  & \\dots \\\\\n",
    "  &\\leq \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3}+ \\gamma^3 R_{t+4} + \\dots \\space | \\space S_t=s \\Big]\\\\\n",
    "  &= v_{\\pi'}(s)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen how, given a policy and its value function, we can easily evaluate a change in the policy at a single state to a perticular action. It is a natural extension to consider changes at all states and to all possible actions, selecting at each state the action that appears best according to $q_\\pi(s,a)$. In other words, to consider the new greedy policy, $\\pi'$ given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\n",
    "\\begin{align}\n",
    "\\pi'(s)  \n",
    "& \\doteq \\underset {a}{argmax} \\space q_\\pi(s,a)\\\\\n",
    "&= \\underset {a}{argmax} \\space \\mathbb{E} \\Big[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\space | \\space S_t=s,A_t=a\\Big] && \\longrightarrow (4.9)\\\\\n",
    "&= \\underset {a}{argmax} \\space \\sum_{s',r}p(s',r\\space \\big| \\space s,a)\\Big[r + \\gamma v_\\pi(s') \\Big]\n",
    "\\end{align}\n",
    "$\n",
    ">\n",
    "Where $\\underset {a}{argmax}$ denotes the value of $a$ at which the expression that follows is maximized (with ties broken arbitrarily). The greed policy takes the action that looks best in the short term after one step of lookahead, according to $v_\\pi$. By construction, the greed policy meets the conditions of the policy improvement theorem (4.7), so we know that it is as good as, or better than, the original policy.\n",
    "The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called **policy impovement**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the new greedy policy, $\\pi'$ is as good as, but not better than, the old policy $\\pi$. Then $v_\\pi = v_{\\pi'}$, and from (4.9) it follows that for all $s \\in S$:\n",
    "\n",
    "$\\large\n",
    "\\begin{split}\n",
    "v_{\\pi'}(s)\n",
    "&= \\underset {a}{max} \\space \\mathbb{E} \\Big[ R_{t+1}+\\gamma v_{\\pi'}(S_{t+1}) \\space | \\space S_t=s,A_t=a\\Big]\\\\\n",
    "&= \\underset {a}{max} \\space \\sum_{s',r}p(s',r|s,a)\\Big[ r + \\gamma v_{\\pi'}(s')\\Big]\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is same as the Bellman optimality equation (4.1), and therefore, $v_{\\pi'}$ must be $v_*$ and both $\\pi$ and $\\pi'$ must be optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4.2: Jack's Car Rental\n",
    "</p>\n",
    "Jack manages two locations for nationwide car rental company. Each day, some number of customers aive at each location to rent cars. If Jack has a car available, he rents it out and is credited $\\$10$ by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at the cost of $\\$2$ per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\\frac{\\lambda^n}{n!}e^{-\\lambda}$, where $\\lambda$ is the expected number. Suppose $\\lambda$ is $3$ and $4$ for rental requests at the first and second locations, and $3$ and $2$ for returns. To simplify the problem slightly we assume that there can be no more than $20$ cars at each location (any additional cars are returned to the nationwide compay, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\\gamma = 0.9$ and formulate this as continuing finite MDP, where the time steps are days, the state is the number of cars at each loction at the end of the day and the actions are the net numbers of cars moved between the two locations overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "$\n",
    "\\text{Analysis}:\\\\\n",
    "\\text{State}: \\text {Number of cars at each location.}\\\\\n",
    "\\text{Action}: \\text{Number of cars moved between two locations.}\\\\\n",
    "\\text{Positive Reward}: 10\\$ \\text{ for each car rented.}\\\\\n",
    "\\text{Negetive Reward}: -2\\$ \\text{ for each car moved between locations.}\\\\ \n",
    "\\lambda_{location1} = 3\\\\\n",
    "\\lambda_{location2} = 4\\\\\n",
    "K_{location1} = 3\\\\\n",
    "K_{location2} = 2\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution: \n",
    "\n",
    "Probability of event k is given by $\\large p(k) = \\frac{\\lambda^k}{k!} e^{-\\lambda}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp, factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoissonDistribution(k,lam):\n",
    "    fract = lam**k / factorial(k)\n",
    "    expVal = exp(-lam)\n",
    "    return fract * expVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36787944117144233\n",
      "0.36787944117144233\n",
      "0.18393972058572117\n",
      "0.061313240195240384\n",
      "0.015328310048810096\n",
      "0.0030656620097620196\n",
      "0.0005109436682936699\n",
      "7.299195261338141e-05\n"
     ]
    }
   ],
   "source": [
    "for k in range(8):\n",
    "    print(PoissonDistribution(k,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets calucate the poisson Distirbution for the following values of \n",
    "$\n",
    "\\lambda_{location1} = 3, \n",
    "\\lambda_{location2} = 4,\n",
    "K_{location1} = 3,\n",
    "K_{location2} = 2,\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3, L = 3, probability = 0.22404180765538775\n",
      "k = 3, L = 4, probability = 0.19536681481316456\n",
      "k = 2, L = 3, probability = 0.22404180765538775\n",
      "k = 2, L = 4, probability = 0.14652511110987343\n"
     ]
    }
   ],
   "source": [
    "K = [3,2]\n",
    "L = [3,4]\n",
    "for k in K:\n",
    "    for l in L:\n",
    "        print(f'k = {k}, L = {l}, probability = {PoissonDistribution(k,l)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_django",
   "language": "python",
   "name": "py37_django"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
