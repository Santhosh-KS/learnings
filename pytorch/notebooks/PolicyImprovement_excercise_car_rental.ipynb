{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function $v_\\pi$ for an arbitrary deterministic policy $\\pi$. For some state s we would like to know whether or not we should change the policiy to deterministically choose an action $a \\neq \\pi(s)$. We know how good it is to follow the current policy from $s$, that is $v_\\pi(s)$ but would it be better or worse to chang to the new policy? One way to answer this question is to consider selecting $a$ in a $s$ and thereafter following the existing policy, $\\pi$. The value of this way of behaving is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\n",
    "\\begin{align}\n",
    "q_\\pi(s,a) \n",
    "  &\\doteq \\mathbb{E}\\Big[ R_{t+1} + \\gamma \\space v_\\pi(S_{t+1}) \\space | S_t=s,A_t=a \\Big]\\\\\n",
    "  &= \\sum_{s',r}p(s',r \\space | \\space s,a) \\Big[ r + \\gamma v_pi(s') \\Big] \\longrightarrow (4.6)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key criterion is whether this is greater than or less than $v_\\pi(s)$. If it is greater, that is if it is better to select $a$ once in $s$ and thereafter follow  $\\pi$ than it would be to follow $\\pi$ all the time, then one would expect it to be better still to select $a$ ever time $s$ is encountered, and that the new policy would in fact be better one overall.\n",
    ">\n",
    "That this is true is a special case of a general result called the **Policy improvement theorem**. Let $\\pi$ and $\\pi'$ be any pair of deterministic policies such that, for all $s \\in S$,\n",
    ">\n",
    "$\\large q_\\pi(s,\\pi'(s)) \\geq \\space v_\\pi(s) \\longrightarrow (4.7)$\n",
    ">\n",
    "Then the policy $\\pi'$ must be as good as, or better than, $\\pi$. That is, it must obtain greater or equal expected return from all state $s \\in S$:\n",
    ">\n",
    "$\\large v_\\pi'(s) \\geq v_\\pi(s) \\longrightarrow (4.8)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, if there is strict inequality of (4.7) at any state, then there must be strict inequality of (4.8 at that state. This result applies in particular to the two policies that we considered in the previous paragraph, an original deterministic policy, $\\pi$, and a changed policy, $\\pi'$, that is identical to $\\pi$ except that $\\large \\pi'(s) = a \\neq \\pi(s)$, (4.7) holds at all states other than $s$. Thus, if $\\large q_\\pi(s,a) > v_\\pi(s)$, then the changed policy is indeed better than $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the proof of **policy improvement therorem** is easy to understand. Starting from (4.7), we keep expanding the $q_\\pi$ side with (4.6) and reapplying (4.7) untill we get $v_\\pi'(s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\n",
    "\\begin{align}\n",
    "v_\\pi(s)\n",
    "  &\\leq q_\\pi(s,\\pi'(s))\\\\\n",
    "  &= \\mathbb{E}\\Big[ R_{t+1} + \\gamma \\space v_\\pi(S_{t+1}) \\space | S_t=s,A_t=a \\Big] && by(4.6) \\\\\n",
    "  &= \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma \\space v_\\pi(S_{t+1}) \\space | S_t=s \\Big]\\\\\n",
    "  &\\leq \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma \\space q_\\pi(S_{t+1}, \\pi'(S_{t+1})) \\space | S_t=s \\Big] && by(4.7) \\\\\n",
    "  &= \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma \\space \\mathbb{E}_{\\pi'}\\Big[ R_{t+2} + \\gamma v_\\pi(S_{t+2}) \\space | \\space S_{t+1},A_{t+1}=\\pi'(S_{t+1})\\Big]  \\space \\Big| \\space S_t=s\\Big]\\\\\n",
    "  &= \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 v_\\pi(S_{t+2})\\space | \\space S_t=s \\Big]\\\\\n",
    "  &= \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3}+ \\gamma^3 v_\\pi(S_{t+3})\\space | \\space S_t=s \\Big]\\\\\n",
    "  & \\dots\\\\\n",
    "  & \\dots \\\\\n",
    "  &\\leq \\mathbb{E}_{\\pi'}\\Big[ R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3}+ \\gamma^3 R_{t+4} + \\dots \\space | \\space S_t=s \\Big]\\\\\n",
    "  &= v_{\\pi'}(s)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have seen how, given a policy and its value function, we can easily evaluate a change in the policy at a single state to a perticular action. It is a natural extension to consider changes at all states and to all possible actions, selecting at each state the action that appears best according to $q_\\pi(s,a)$. In other words, to consider the new greedy policy, $\\pi'$ given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large\n",
    "\\begin{align}\n",
    "\\pi'(s)  \n",
    "& \\doteq \\underset {a}{argmax} \\space q_\\pi(s,a)\\\\\n",
    "&= \\underset {a}{argmax} \\space \\mathbb{E} \\Big[ R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\space | \\space S_t=s,A_t=a\\Big] && \\longrightarrow (4.9)\\\\\n",
    "&= \\underset {a}{argmax} \\space \\sum_{s',r}p(s',r\\space \\big| \\space s,a)\\Big[r + \\gamma v_\\pi(s') \\Big]\n",
    "\\end{align}\n",
    "$\n",
    ">\n",
    "Where $\\underset {a}{argmax}$ denotes the value of $a$ at which the expression that follows is maximized (with ties broken arbitrarily). The greed policy takes the action that looks best in the short term after one step of lookahead, according to $v_\\pi$. By construction, the greed policy meets the conditions of the policy improvement theorem (4.7), so we know that it is as good as, or better than, the original policy.\n",
    "The process of making a new policy that improves on an original policy, by making it greedy with respect to the value function of the original policy, is called **policy impovement**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the new greedy policy, $\\pi'$ is as good as, but not better than, the old policy $\\pi$. Then $v_\\pi = v_{\\pi'}$, and from (4.9) it follows that for all $s \\in S$:\n",
    "\n",
    "$\\large\n",
    "\\begin{split}\n",
    "v_{\\pi'}(s)\n",
    "&= \\underset {a}{max} \\space \\mathbb{E} \\Big[ R_{t+1}+\\gamma v_{\\pi'}(S_{t+1}) \\space | \\space S_t=s,A_t=a\\Big]\\\\\n",
    "&= \\underset {a}{max} \\space \\sum_{s',r}p(s',r|s,a)\\Big[ r + \\gamma v_{\\pi'}(s')\\Big]\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is same as the Bellman optimality equation (4.1), and therefore, $v_{\\pi'}$ must be $v_*$ and both $\\pi$ and $\\pi'$ must be optimal policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4.2: Jack's Car Rental\n",
    "</p>\n",
    "Jack manages two locations for nationwide car rental company. Each day, some number of customers aive at each location to rent cars. If Jack has a car available, he rents it out and is credited $\\$10$ by the national company. If he is out of cars at that location, then the business is lost. Cars become available for renting the day after they are returned. To help ensure that cars are available where they are needed, Jack can move them between the two locations overnight, at the cost of $\\$2$ per car moved. We assume that the number of cars requested and returned at each location are Poisson random variables, meaning that the probability that the number is $n$ is $\\frac{\\lambda^n}{n!}e^{-\\lambda}$, where $\\lambda$ is the expected number. Suppose $\\lambda$ is $3$ and $4$ for rental requests at the first and second locations, and $3$ and $2$ for returns. To simplify the problem slightly we assume that there can be no more than $20$ cars at each location (any additional cars are returned to the nationwide compay, and thus disappear from the problem) and a maximum of five cars can be moved from one location to the other in one night. We take the discount rate to be $\\gamma = 0.9$ and formulate this as continuing finite MDP, where the time steps are days, the state is the number of cars at each loction at the end of the day and the actions are the net numbers of cars moved between the two locations overnight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">\n",
    "$\n",
    "\\text{Analysis}:\\\\\n",
    "\\text{State}: \\text {Number of cars at each location.}\\\\\n",
    "\\text{Action}: \\text{Number of cars moved between two locations.}\\\\\n",
    "\\text{Positive Reward}: 10\\$ \\text{ for each car rented.}\\\\\n",
    "\\text{Negetive Reward}: -2\\$ \\text{ for each car moved between locations.}\\\\ \n",
    "\\lambda_{location1} = 3\\\\\n",
    "\\lambda_{location2} = 4\\\\\n",
    "K_{location1} = 3\\\\\n",
    "K_{location2} = 2\\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson Distribution: \n",
    "\n",
    "Probability of event k is given by $\\large p(k) = \\frac{\\lambda^k}{k!} e^{-\\lambda}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import exp, factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PoissonDistribution(k,lam):\n",
    "    fract = lam**k / factorial(k)\n",
    "    expVal = exp(-lam)\n",
    "    return fract * expVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36787944117144233\n",
      "0.36787944117144233\n",
      "0.18393972058572117\n",
      "0.061313240195240384\n",
      "0.015328310048810096\n",
      "0.0030656620097620196\n",
      "0.0005109436682936699\n",
      "7.299195261338141e-05\n"
     ]
    }
   ],
   "source": [
    "for k in range(8):\n",
    "    print(PoissonDistribution(k,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets calucate the poisson Distirbution for the following values of \n",
    "$\n",
    "\\lambda_{location1} = 3, \n",
    "\\lambda_{location2} = 4,\n",
    "K_{location1} = 3,\n",
    "K_{location2} = 2,\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 3, L = 3, probability = 0.22404180765538775\n",
      "k = 3, L = 4, probability = 0.19536681481316456\n",
      "k = 2, L = 3, probability = 0.22404180765538775\n",
      "k = 2, L = 4, probability = 0.14652511110987343\n"
     ]
    }
   ],
   "source": [
    "K = [3,2]\n",
    "L = [3,4]\n",
    "for k in K:\n",
    "    for l in L:\n",
    "        print(f'k = {k}, L = {l}, probability = {PoissonDistribution(k,l)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from math import exp, factorial\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum # of cars in each location\n",
    "MAX_CARS = 20\n",
    "\n",
    "# maximum # of cars to move during night\n",
    "MAX_MOVE_OF_CARS = 5\n",
    "\n",
    "# expectation for rental requests in first location\n",
    "RENTAL_REQUEST_FIRST_LOC = 3\n",
    "\n",
    "# expectation for rental requests in second location\n",
    "RENTAL_REQUEST_SECOND_LOC = 4\n",
    "\n",
    "# expectation for # of cars returned in first location\n",
    "RETURNS_FIRST_LOC = 3\n",
    "\n",
    "# expectation for # of cars returned in second location\n",
    "RETURNS_SECOND_LOC = 2\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "# credit earned by a car\n",
    "RENTAL_CREDIT = 10\n",
    "\n",
    "# cost of moving a car\n",
    "MOVE_CAR_COST = 2\n",
    "\n",
    "# all possible actions\n",
    "actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)\n",
    "\n",
    "# An up bound for poisson distribution\n",
    "# If n is greater than this value, then the probability of getting n is truncated to 0\n",
    "POISSON_UPPER_BOUND = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Probability for poisson distribution\n",
    "# @lam: lambda should be less than 10 for this function\n",
    "\"\"\"\n",
    "poisson_cache = dict()\n",
    "def poisson(n, lam):\n",
    "    global poisson_cache\n",
    "    key = n * 10 + lam\n",
    "    if key not in poisson_cache.keys():\n",
    "        poisson_cache[key] = exp(-lam) * pow(lam, n) / factorial(n)\n",
    "    return poisson_cache[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " @state: [# of cars in first location, # of cars in second location]\n",
    " @action: positive if moving cars from first location to second location,\n",
    "          negative if moving cars from second location to first location\n",
    " @stateValue: state value matrix\n",
    " @constant_returned_cars:  if set True, model is simplified such that\n",
    "   the # of cars returned in daytime becomes constant\n",
    "   rather than a random value from poisson distribution, which will reduce calculation time\n",
    "   and leave the optimal policy/value state matrix almost the same\n",
    "\"\"\"\n",
    "\n",
    "def ExpectedReturn(state, action, state_value, constant_returned_cars):\n",
    "    \n",
    "    returns = 0.0\n",
    "    \n",
    "    #cost for moving cars.\n",
    "    returns -= MOVE_CAR_COST * abs(action)\n",
    "    \n",
    "    #go through all possible rental requests\n",
    "    for rental_request_first_loc in range(0,POISSON_UPPER_BOUND):\n",
    "        for rental_request_second_loc in range(0, POISSON_UPPER_BOUND):\n",
    "            \n",
    "            #moving cars\n",
    "            num_of_cars_first_loc = int(min(state[0] - action, MAX_CARS))\n",
    "            num_of_cars_second_loc = int(min(state[1] + action, MAX_CARS))\n",
    "            \n",
    "            #valid rental requests should be less than the actual #of cars\n",
    "            real_rental_first_loc = min(num_of_cars_first_loc, rental_request_first_loc)\n",
    "            real_rental_second_loc = min(num_of_cars_second_loc, rental_request_second_loc)\n",
    "            \n",
    "            #get credit for renting.\n",
    "            reward = (real_rental_first_loc + real_rental_second_loc) * RENTAL_CREDIT\n",
    "            num_of_cars_first_loc -= real_rental_first_loc\n",
    "            num_of_cars_second_loc -= real_rental_second_loc\n",
    "            \n",
    "            #probability for current combination of rental requests\n",
    "            prob = poisson(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC) * \\\n",
    "                   poisson(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC)\n",
    "            \n",
    "            if constant_returned_cars:\n",
    "                # get returned cars, those cars can be used for renting tomorrow\n",
    "                returned_cars_first_loc = RENTAL_REQUEST_FIRST_LOC\n",
    "                returned_cars_second_loc = RENTAL_REQUEST_SECOND_LOC\n",
    "                num_of_cars_first_loc = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n",
    "                num_of_cars_second_loc = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n",
    "                returns += prob * (reward + DISCOUNT * state_value[num_of_cars_first_loc, num_of_cars_second_loc])\n",
    "            else:\n",
    "                for returned_cars_first_loc in range(0, POISSON_UPPER_BOUND):\n",
    "                    for returned_cars_second_loc in range(0, POISSON_UPPER_BOUND):\n",
    "                        num_of_cars_fist_loc_ = min(num_of_cars_fist_loc + returned_cars_first_loc, MAX_CARS)\n",
    "                        num_of_cars_second_loc_ = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n",
    "                        \n",
    "                        prob_ = poisson(returned_cars_first_loc, RETURNS_FIRST_LOC) * \\\n",
    "                                poisson(returned_cars_second_loc, RETURNS_SECOND_LOC) * prob\n",
    "                        \n",
    "                        returns += prob_ *(reward + DISCOUNT * state_value[num_of_cars_first_loc_, num_of_cars_second_loc_])\n",
    "            \n",
    "    return returns        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21, 21), (21, 21))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "policy = np.zeros(value.shape, dtype=np.int)\n",
    "\n",
    "iterations = 0\n",
    "policy.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value change 1443.2331513808886\n",
      "value change 0.0\n",
      "policy changed in 349\n",
      "value change 96.39157017805087\n",
      "value change 0.0\n",
      "policy changed in 7\n",
      "value change 0.015681404415722966\n",
      "value change 0.0\n",
      "policy changed in 0\n"
     ]
    }
   ],
   "source": [
    "value = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "policy = np.zeros(value.shape, dtype=np.int)\n",
    "\n",
    "iterations = 0\n",
    "policy.shape, value.shape\n",
    "constant_returned_cars = True\n",
    "\n",
    "_, axes = plt.subplots(2, 3, figsize=(40, 20))\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "axes = axes.flatten()\n",
    "\n",
    "\n",
    "while True:\n",
    "    fig = sns.heatmap(np.flipud(policy), cmap=\"YlGnBu\", ax=axes[iterations])\n",
    "    fig.set_ylabel('# cars at first location', fontsize=30)\n",
    "    fig.set_yticks(list(reversed(range(MAX_CARS + 1))))\n",
    "    fig.set_xlabel('# cars at second location', fontsize=30)\n",
    "    fig.set_title('policy %d' % (iterations), fontsize=30)\n",
    "\n",
    "    # policy evaluation inplace.\n",
    "    while True:\n",
    "        new_value = np.copy(value)\n",
    "        for i in range(MAX_CARS + 1):\n",
    "            for j in range(MAX_CARS + 1):\n",
    "                new_value[j,j] = ExpectedReturn([i,j], policy[i,j], new_value, constant_returned_cars)\n",
    "        value_change = np.abs((new_value - value)).sum()\n",
    "        print(f'value change {value_change}')\n",
    "        value  = new_value\n",
    "        if value_change < 1e-4:\n",
    "            break\n",
    "            \n",
    "    # policy improvement.\n",
    "    new_policy = np.copy(policy)\n",
    "    for i in range(MAX_CARS + 1):\n",
    "        for j in range(MAX_CARS + 1):\n",
    "            action_returns = []\n",
    "            for action in actions:\n",
    "                if (action >=0 and i >=0) or (action < 0 and j >= abs(action)):\n",
    "                    action_returns.append(ExpectedReturn([i,j], action, value, constant_returned_cars))\n",
    "                else:\n",
    "                    action_returns.append(-float('inf'))\n",
    "            new_policy[i,j] =actions[np.argmax(action_returns)]\n",
    "    policy_change = (new_policy != policy).sum()\n",
    "    print(f'policy changed in {policy_change}')\n",
    "    policy = new_policy\n",
    "    \n",
    "    if policy_change == 0:\n",
    "        fig = sns.heatmap(np.flipud(value), cmap=\"YlGnBu\", ax=axes[-1])\n",
    "        fig.set_ylabel('# cars at first location', fontsize=30)\n",
    "        fig.set_yticks(list(reversed(range(MAX_CARS + 1))))\n",
    "        fig.set_xlabel('# cars at second location', fontsize=30)\n",
    "        fig.set_title('optimal value', fontsize=30)\n",
    "        break\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_django",
   "language": "python",
   "name": "py37_django"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
