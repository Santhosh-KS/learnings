{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Policies and Optimal Value Functions:\n",
    "\n",
    "Solving a reinforcement learning tarsk means roughly finding a policy that achieves a lot of reward over the long run. For finite MDPs, we can precisely define an optimal policy in the following way.\n",
    "\n",
    "Value functions define a partial ordering over policies. A policy $\\pi$ is defined to be better than the equal to a policy $\\pi'$, if its expected return is greater than or equal to that of $\\pi'$ for all states. In other words, $\\pi \\geq \\pi'$ if and only if $v_\\pi(s) \\geq v_{\\pi'}(s)$ for all $s \\in S$. There is always atleast one policy that is better than or equal to all other policies. This is an **optimal policy**. Although there may be more than one, we denote all the optimal policies by $\\pi_*$. They share the same state-value function, called the optimal state-value function, denoted $v_*$, and defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\large v_*(s) \\doteq \\underset {\\pi}{max} \\space v_\\pi(s)$$\n",
    "</p>\n",
    "for all $s \\in S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal policies also share the same **optimal action-value function**, denated $q_*$, and defined as\n",
    "\n",
    "$$\\large q_* \\doteq \\underset {\\pi}{max} \\space q_\\pi(s,a)$$\n",
    "\n",
    "for all $s \\in S$ and $a \\in A(s)$\n",
    "\n",
    "For the state-action pair(s,a), this function gives the expected return for taking action $a$ in state $s$ and thereafter following an optimal policy. Thus, we can write $q_*$ in terms of $v)*$ as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large q_*(s,a) =  \\mathbb{E}[R_{t+1} + \\gamma  v_*(S_{t+1})\\space | \\space S_t=s, A_t=a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because $v_x$ is the value function for a policy, it must satisfy the self-consistency condtion given by the Bellman equation for state values. Buecause it is the optimal value function, however, $v_*$'s consistency condition can be written in a special form without reference to any specific policy. This is the Bellman equation for $v_*$, or the **Bellman optimality equation**. Intutively, the Bellman optimality equation expresses the fact that the value of a state under an optimal policy must equal the expected return for the best action from that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large v_*(s) = \\underset {a \\in A(s)}{max} \\space q_{\\pi_*}(s,a)$\n",
    "</p>\n",
    "\n",
    "$\\large  = \\underset{a}{max}\\mathbb{E}_{\\pi_*}[G_t \\space | \\space S_t=s, A_t=a]$\n",
    "\n",
    "$\\large  = \\underset{a}{max}\\mathbb{E}_{\\pi_*}[R_{t+1} + \\gamma G_{t+1} \\space | \\space S_t=s, A_t=a]$\n",
    "\n",
    "$\\large  = \\underset{a}{max}\\mathbb{E}_{\\pi_*}[R_{t+1} + \\gamma v_*(S_{t+1}) \\space | \\space S_t=s, A_t=a] \\rightarrow (3.18)$\n",
    "\n",
    "$\\large  = \\underset{a}{max} \\sum_{s',r'} p(s',r \\space | \\space s,a)\\big[ r + \\gamma v_*(s')\\big] \\rightarrow (3.19)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the last two equations are two forms of the Bellman optimality equation for $v_*$. The Bellman optimailty equation for $q_*$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large q_*(s,a) = \\mathbb{E}\\Big[ R_{t+1} + \\gamma \\space \\underset {a'}{max} \\space q_*(S_{t+1},a') \\space | \\space S_t =s, A_t=a \\Big]$$\n",
    "$$\\large =\\sum_{s',r}p(s',r |s,a)\\Big[ r + \\gamma \\space \\underset {a'}{max} \\space q_*(s',a')\\Big] \\rightarrow (3.20)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finite MDPs, the Bellman optimality equation for $v_*$ (3.19) has a unique solution. The Bellman optimality equation is actually a system of equations, one for each state, so if there are n states, then there are $n$ equations in $n$ unknowns. If the dynamics $p$ of the environment are known, then in principle once can solve this system of equations for $v_*$ using any one of variety of methods for solving systems of nonlinear equations. One can solve a related set of equations for $q_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once one has $v_*$, it is relatively easy to determine an optimal policy. For each state $s$, there will be one or more actions at which the maximum is obtained in the Bellman optimality equation. Any policy that assigns nonzero probability only to these actions is an optimal policy. You can think of this as one-step search.\n",
    "If you have the optimal value function, $v_*$, then the actions that appear best after a one-step search will be optimal actions.\n",
    "Another way of saying this is that any policy that is **greedy** with respect to the optimal evaluation function $v_*$ is an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term **greedy** is used in computer science to describe any search or decision procedure that selects alternatives based only on local or immediate considerations, without considering the possibility that such selection my prevent future access to even better alternatives. Consequently, it describes policies that select actions based only on their short-term consequences. The beauty of $v_*$ is that if one uses it to evaluate the short-term consequences of actions-specifically, the one-step consequences, then a greedy policy is actually optimal in the long-term sense in which we are interested because $v_*$ already takes into account the reward consequences of all possible future behaviour. Bye means of $v_*$, the optimal expected long-term return is turned into a quantity that is locally and immediately available for each state. Hence, a one-step-ahead search yeilds the long-term optimal actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having $q_*$ makes choosing optimal actions even easier. With $q_*$, the agent does not even have to do a one-step-ahead search. For any state $s$, it can simply find any action that maximizes $q_*(s,a). The action-value function effectively caches the results of all one-step-ahead searches. It provides the optimal expected long-term return as a vlaue that is locally and immediately available for each state-action pair. Hence, at the cost of representing a function of state-action pairs, instread of just of states, the optimal action-value function allows optimal actions to be selected without having to know anything about possible successor states and their values, that is without having to know anything about the environment's dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.24**\n",
    "</p>\n",
    "Gives the optimal value of the best state of the gridworld as\n",
    "24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express\n",
    "this value symbolically, and then to compute it to three decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_SIZE = 5\n",
    "\n",
    "A_POS = [0,1]\n",
    "B_POS = [0,3]\n",
    "\n",
    "A_PRIME_POS = [4,1]\n",
    "B_PRIME_POS = [2,3]\n",
    "\n",
    "A_TO_APRIME_TRANSITION_REWARD = 10\n",
    "B_TO_BPRIME_TRANSITION_REWARD = 5\n",
    "\n",
    "Gamma = 0.9\n",
    "\n",
    "# left, up, right down\n",
    "Actions =[np.array([0,-1]),\n",
    "          np.array([-1,0]),\n",
    "          np.array([0,1]),\n",
    "          np.array([1,0])]\n",
    "\n",
    "#In the problem it is given that actions are equiprobable.\n",
    "#1/4 = 0.25\n",
    "ACTION_PROB =  1/len(Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Step(state, action):\n",
    "    \n",
    "    if state == A_POS:\n",
    "        return A_PRIME_POS, A_TO_APRIME_TRANSITION_REWARD\n",
    "    elif state == B_POS:\n",
    "        return B_PRIME_POS, B_TO_BPRIME_TRANSITION_REWARD\n",
    "    else:\n",
    "        state = np.array(state)\n",
    "        next_state = state + action\n",
    "        x, y = next_state\n",
    "        if x < 0 or x >= WORLD_SIZE or y < 0 or y >= WORLD_SIZE:\n",
    "            reward = -1\n",
    "            next_state = state\n",
    "        else :\n",
    "            reward = 0\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value got converged after 124 iterations\n",
      "[[21.97744338 24.41938153 21.97744338 19.41938153 17.47744338]\n",
      " [19.77969904 21.97744338 19.77969904 17.8017056  16.02153504]\n",
      " [17.8017056  19.77969904 17.8017056  16.02153504 14.41938153]\n",
      " [16.02153504 17.8017056  16.02153504 14.41938153 12.97744338]\n",
      " [14.41938153 16.02153504 14.41938153 12.97744338 11.67969904]]\n"
     ]
    }
   ],
   "source": [
    "values = np.zeros((WORLD_SIZE,WORLD_SIZE))\n",
    "values, values.shape\n",
    "count = 0\n",
    "while True:\n",
    "    # keep iterating until convergence\n",
    "    count += 1\n",
    "    new_value = np.zeros(values.shape,)\n",
    "    for i in range(0, WORLD_SIZE):\n",
    "        for j in range(0, WORLD_SIZE):\n",
    "            all_val = []\n",
    "            for action in Actions:\n",
    "                (next_i, next_j), reward = Step([i,j], action)\n",
    "                # value iteration\n",
    "                all_val.append(reward + Gamma * values[next_i,next_j])\n",
    "            new_value[i,j] = np.max(all_val)\n",
    "    if np.sum(np.abs(values - new_value)) < 1e-4:\n",
    "        print(f'Value got converged after {count} iterations')\n",
    "        print(values)\n",
    "        break\n",
    "    values = new_value\n",
    "    #print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_django",
   "language": "python",
   "name": "py37_django"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
