{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3.5**\n",
    "</p>\n",
    "Gridworld Figure (left) shows a rectangular gridworld representation\n",
    "of a simple finite MDP. The cells of the grid correspond to the states of the environment. At\n",
    "each cell, four actions are possible: $north$, $south$, $east$, and $west$, which deterministically\n",
    "cause the agent to move one cell in the respective direction on the grid. Actions that\n",
    "would take the agent off the grid leave its location unchanged, but also result in a reward\n",
    "of $-1$. Other actions result in a reward of $0$, except those that move the agent out of the\n",
    "special $states A$ and $B$. From state $A$, all four actions yield a reward of $+10$ and take the\n",
    "agent to $A'$. From state $B$, all actions yield a reward of +5 and take the agent to $B'$\n",
    "\n",
    "Suppose the agent selects all **four actions with equal probability** in all states. Table shows the value function, $\\large v_\\pi$, for this policy, for the discounted reward case with\n",
    "$\\large \\gamma$ = 0.9. This value function was computed by solving the system of linear equations\n",
    "$$\\large v_\\pi(s) \\doteq \\sum_{a} \\pi(a|s) \\sum_{s',r}p(s',r | s, a) \\Big[ r + \\gamma \\space v_{\\pi}(s')\\Big] \\to for \\space all \\space s \\in S$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|a|b|c|d|e|\n",
    "|---|-----|----|----|----|\n",
    "|3.3| 8.8 | 4.4| 5.3| 1.5|\n",
    "|1.5 |3.0 |2.3 |1.9 |0.5|\n",
    "|0.1 |0.7 | 0.7 |0.4 |-0.4|\n",
    "|-1.0 |-0.4| -0.4| -0.6 |-1.2|\n",
    "|-1.9| -1.3| -1.2| -1.4 |-2.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|a|b|c|d|e|\n",
    "|---|-----|----|----|----|\n",
    "|3.3| 8.8 | 4.4| 5.3| 1.5|\n",
    "|1.5 |3.0 |2.3 |1.9 |0.5|\n",
    "|0.1 |0.7 | 0.7 |0.4 |-0.4|\n",
    "|-1.0 |-0.4| -0.4| -0.6 |-1.2|\n",
    "|-1.9| -1.3| -1.2| -1.4 |-2.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Notice the negative values near the lower edge; these are the result of the high\n",
    "probability of hitting the edge of the grid there under the random policy. State A is the\n",
    "best state to be in under this policy, but its expected return is less than 10, its immediate\n",
    "reward, because from $A$ the agent is taken to $A'$, from which it is likely to run into the\n",
    "edge of the grid. State $B$, on the other hand, is valued more than 5, its immediate reward,\n",
    "because from B the agent is taken to $B'$, which has a positive value. From $B'$ the expected\n",
    "penalty (negative reward) for possibly running into an edge is more than compensated\n",
    "for by the expected gain for possibly stumbling onto $A$ or $B$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "#from matplotlib.table import Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORLD_SIZE = 5\n",
    "\n",
    "A_POS = [0,1]\n",
    "B_POS = [0,3]\n",
    "\n",
    "A_PRIME_POS = [4,1]\n",
    "B_PRIME_POS = [2,3]\n",
    "\n",
    "A_TO_APRIME_TRANSITION_REWARD = 10\n",
    "B_TO_BPRIME_TRANSITION_REWARD = 5\n",
    "\n",
    "Gamma = 0.9\n",
    "\n",
    "# left, up, right down\n",
    "Actions =[np.array([0,-1]),\n",
    "          np.array([-1,0]),\n",
    "          np.array([0,1]),\n",
    "          np.array([1,0])]\n",
    "\n",
    "#In the problem it is given that actions are equiprobable.\n",
    "#1/4 = 0.25\n",
    "ACTION_PROB =  1/len(Actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Step(state, action):\n",
    "    \n",
    "    if state == A_POS:\n",
    "        return A_PRIME_POS, A_TO_APRIME_TRANSITION_REWARD\n",
    "    elif state == B_POS:\n",
    "        return B_PRIME_POS, B_TO_BPRIME_TRANSITION_REWARD\n",
    "    else:\n",
    "        state = np.array(state)\n",
    "        next_state = state + action\n",
    "        x, y = next_state\n",
    "        if x < 0 or x > WORLD_SIZE or y < 0 or y >WORLD_SIZE:\n",
    "            reward = -1\n",
    "            next_state = state\n",
    "        else :\n",
    "            reward = 0\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4] + [ 0 -1] = [1 3]\n",
      "[1 4] + [-1  0] = [0 4]\n",
      "[1 4] + [0 1] = [1 5]\n",
      "[1 4] + [1 0] = [2 4]\n"
     ]
    }
   ],
   "source": [
    "test_state =np.array( [1,4])\n",
    "\n",
    "for a in Actions:\n",
    "    n_s = test_state + a\n",
    "    print(f'{test_state} + {a} = {n_s}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = test_state\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 4] 0\n",
      "[0 5] -1\n",
      "[0 5] -1\n",
      "[1 5] 0\n"
     ]
    }
   ],
   "source": [
    "test_state =[0,5]\n",
    "\n",
    "for a in Actions:\n",
    "    m_test, r = Step(test_state,a)\n",
    "    print(f'{m_test} {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value got converged after 24 iterations\n",
      "[[ 4.01 10.    5.02  5.72  0.  ]\n",
      " [ 2.06  3.6   2.67  2.07  0.  ]\n",
      " [ 0.63  1.29  1.16  0.79  0.  ]\n",
      " [-0.04  0.37  0.4   0.27  0.  ]\n",
      " [ 0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "values = np.zeros((WORLD_SIZE,WORLD_SIZE))\n",
    "values, values.shape\n",
    "count = 0\n",
    "while True:\n",
    "    # keep iteration until convergence\n",
    "    count += 1\n",
    "    new_value = np.zeros(values.shape,)\n",
    "    for i in range(values.shape[0]-1):\n",
    "        for j in range(values.shape[1]-1):\n",
    "            for action in Actions:\n",
    "                (next_i, next_j), reward = Step([i,j], action)\n",
    "                # Bellman Equation.\n",
    "                new_value[i,j]  = round(new_value[i,j] + ACTION_PROB * (reward + Gamma * values[next_i,next_j]),2)\n",
    "                #new_value[i,j]  += ACTION_PROB * (reward + Gamma * values[next_i,next_j])\n",
    "    if np.sum(np.abs(values - new_value)) < 1e-4:\n",
    "        print(f'Value got converged after {count} iterations')\n",
    "        print(values)\n",
    "        break\n",
    "    values = new_value\n",
    "    #print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_django",
   "language": "python",
   "name": "py37_django"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
